{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNyZ-zZxlU6G"
   },
   "source": [
    "# Neural Networks with Tensorflow\n",
    "\n",
    "In this assignment, we are going to train a Neural Networks on the Japanese MNIST dataset. It is composed of 70000 images of handwritten Hiragana characters. The target variables has 10 different classes.\n",
    "\n",
    "Each image is of dimension 28 by 28. But we will flatten them to form a dataset composed of vectors of dimension (784, 1). The training process will be similar as for a structured dataset.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=16TqEl9ESfXYbUpVafXD6h5UpJYGKfMxE' width=\"500\" height=\"200\">\n",
    "\n",
    "Your goal is to run at least 3 experiments and get a model that can achieve 80% accuracy with not much overfitting on this dataset.\n",
    "\n",
    "Some of the code have already been defined for you. You need only to add your code in the sections specified (marked with **TODO**). Some assert statements have been added to verify the expected outputs are correct. If it does throw an error, this means your implementation is behaving as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOufKqO8mw7n"
   },
   "source": [
    "# 1. Import Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-sGJ26pmz4A"
   },
   "source": [
    "[1.1] We are going to use numpy, matplotlib and google.colab packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTGG80etnMAa"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vyky0K3fnEFO"
   },
   "source": [
    "# 2. Download Dataset\n",
    "\n",
    "We will store the dataset into your personal Google Drive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltUMtjG-nX-b"
   },
   "source": [
    "[2.1] Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_FVrXICnMJM"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzLtlKCHnT9H"
   },
   "source": [
    "[2.2] Create a folder called `DL_ASG_1` on your Google Drive at the root level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZicoPks4POW"
   },
   "outputs": [],
   "source": [
    "! mkdir -p /content/gdrive/MyDrive/DL_ASG_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sToej_3CnePP"
   },
   "source": [
    "[2.3] Navigate to this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2oAXToKnpXj"
   },
   "outputs": [],
   "source": [
    "% cd /content/gdrive/MyDrive/DL_ASG_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vlfobqnnjJ1"
   },
   "source": [
    "[2.4] Dowload the dataset files to your Google Drive if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0owzTC427NM"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "\n",
    "def download_file(url):\n",
    "    path = url.split('/')[-1]\n",
    "    if os.path.isfile(path):\n",
    "        print (f\"{path} already exists\")\n",
    "    else:\n",
    "      r = requests.get(url, stream=True)\n",
    "      with open(path, 'wb') as f:\n",
    "          total_length = int(r.headers.get('content-length'))\n",
    "          print('Downloading {} - {:.1f} MB'.format(path, (total_length / 1024000)))\n",
    "          for chunk in tqdm(r.iter_content(chunk_size=1024), total=int(total_length / 1024) + 1, unit=\"KB\"):\n",
    "              if chunk:\n",
    "                  f.write(chunk)\n",
    "\n",
    "url_list = [\n",
    "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz',\n",
    "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz',\n",
    "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz',\n",
    "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz'\n",
    "]\n",
    "\n",
    "for url in url_list:\n",
    "    download_file(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVF_Cx7Hny2i"
   },
   "source": [
    "[2.5] List the content of the folder and confirm files have been dowloaded properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vt6ZKf4fnqkq"
   },
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvvfOON36hTf"
   },
   "source": [
    "# 3. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duFjgsyPoLPR"
   },
   "source": [
    "[3.1] Import the required modules from Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zolHKEO7GZA"
   },
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# extra packages utilised\n",
    "\n",
    "# plot model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# confusion matrix packages\n",
    "from sklearn.metrics import confusion_matrix # for confusion matrix\n",
    "import seaborn as sns # for confusion matrix readability\n",
    "\n",
    "# Early learning stop and saving best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# Convolutional neural network stuff\n",
    "from keras.layers import Conv2D, Lambda, MaxPooling2D # convolution layers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Synthetic data creation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# time execution\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4Aw5ObQoWdI"
   },
   "source": [
    "[3.2] **TODO** Create 2 variables called `img_height` and `img_width` that will both take the value 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ip0NFeyjpj79"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "img_height = 28\n",
    "img_width = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmX5SEHkpp63"
   },
   "source": [
    "[3.3] Create a function that loads a .npz file using numpy and return the content of the `arr_0` key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5S3cthx57L2f"
   },
   "outputs": [],
   "source": [
    "def load(f):\n",
    "    return np.load(f)['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8V2Ij9s7qRtj"
   },
   "source": [
    "[3.4] **TODO** Load the 4 files saved on your Google Drive into their respective variables: x_train, y_train, x_test and y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XTkRb0lqpEE"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "x_train = load('kmnist-train-imgs.npz')\n",
    "x_test = load('kmnist-test-imgs.npz')\n",
    "y_train = load('kmnist-train-labels.npz')\n",
    "y_test = load('kmnist-test-labels.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KC12nB7rlbV"
   },
   "source": [
    "[3.5] **TODO** Using matplotlib display the first image from the train set and its target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOtWg7bBrwmV"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "\n",
    "plt.imshow(x_train[0])\n",
    "print('Label', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htLk_27ir0B1"
   },
   "source": [
    "# 4. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJEBe30Er33P"
   },
   "source": [
    "[4.1] **TODO** Reshape the images from the training and testing set to have the channel dimension last. The dimensions should be: (row_number, height, width, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdi109Tvt2xI"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "x_train = np.reshape(x_train,(x_train.shape[0],img_height,img_width,1))\n",
    "x_test = np.reshape(x_test,(x_test.shape[0],img_height,img_width,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2f6wvFys2ZI"
   },
   "source": [
    "[4.2] **TODO** Cast `x_train` and `x_test` into `float32` decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWZmWe73tLXT"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-1Jr0pKs6jv"
   },
   "source": [
    "[4.3] **TODO** Standardise the images of the training and testing sets. Originally each image contains pixels with value ranging from 0 to 255. after standardisation, the new value range should be from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXY1o272t0JO"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "\n",
    "# grayscale\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eH4aZmXt7Fe"
   },
   "source": [
    "[4.4] **TODO** Create a variable called `num_classes` that will take the value 10 which corresponds to the number of classes for the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTnMgLxYuUs6"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLuuy3XW4xhW"
   },
   "source": [
    "Get first occurances of each character type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qTRSM8NyzBi"
   },
   "outputs": [],
   "source": [
    "# create aray of zeros\n",
    "list_of_positions = np.zeros((num_classes,), dtype=int)\n",
    "\n",
    "# find first occurance of each letter type by number label\n",
    "for i in range(10): # number 0 -9 = 10 unique numbers\n",
    "  result = np.where(y_train == i) # see which y_train label has the number we want\n",
    "  list_of_positions[i] = result[0][0] # get first occurance and change initial array value to that index value\n",
    "\n",
    "# create figure 5x2 for showing images\n",
    "fig=plt.figure(figsize=(num_classes,7))\n",
    "columns = 5\n",
    "rows = 2\n",
    "\n",
    "# create subplots\n",
    "for i in range(num_classes):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    plt.imshow(x_train[list_of_positions[i]].squeeze(), cmap=plt.get_cmap('gray')) # black and white\n",
    "    plt.axis('off') # turn off axis\n",
    "    plt.title(i)  # label just needs to be i as thats how the list was created\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAy0fUJsuyhb"
   },
   "source": [
    "[4.5] **TODO** Convert the target variable for the training and testing sets to a binary class matrix of dimension (rows, num_classes).\n",
    "\n",
    "For example:\n",
    "- class 0 will become [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n",
    "- class 1 will become [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] \n",
    "- class 5 will become [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] \n",
    "- class 9 will become [0, 0, 0, 0, 0, 0, 0, 0, 0, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysNg37Ukwq8S"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "\n",
    "# converts labels to one hot encoder format using np.eye\n",
    "y_train = np.eye(num_classes)[y_train]\n",
    "y_test = np.eye(num_classes)[y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OCorS00wxPN"
   },
   "source": [
    "# 5. Define Neural Networks Architecure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G_L-yqTxI1d"
   },
   "source": [
    "[5.1] Set the seed for Tensorflow Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XB8OIC9wrgFG"
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b93U4MixWeE"
   },
   "source": [
    "[5.2] **TODO** Define the architecture of your Neural Networks and save it into a variable called `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gq1f74uKxpkp"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "\n",
    "model_naive = Sequential() # seems to be widely used\n",
    "\n",
    "# Input layer  \n",
    "model_naive.add(keras.Input(shape=(28, 28, 1)))\n",
    "model_naive.add(Flatten())\n",
    "\n",
    "# 1st hidden layer\n",
    "model_naive.add(Dense(512, activation=\"relu\"))\n",
    "\n",
    "#  2nd hidden layer\n",
    "model_naive.add(Dense(512, activation=\"relu\"))\n",
    "\n",
    "# Final  layer\n",
    "model_naive.add(Dense(num_classes,activation=\"softmax\"))\n",
    "# To get a probability distribution, values should add to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IvuMQ81xu5U"
   },
   "source": [
    "[5.2] **TODO** Print the summary of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBRm-h5dxvIw"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "model_naive.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBF4AalesvME"
   },
   "outputs": [],
   "source": [
    "# plot model_naive \n",
    "plot_model(model_naive, to_file='model__naive_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOPTnNxtx6MC"
   },
   "source": [
    "# 6. Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsHJzhnAyP4H"
   },
   "source": [
    "[6.1] **TODO** Create 2 variables called `batch_size` and `epochs` that will  respectively take the values 128 and 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNe_Cia0yde-"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "batch_size = 128\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-bAkzwXyjAs"
   },
   "source": [
    "[6.2] **TODO** Compile your model with the appropriate loss function, the optimiser of your choice and the accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WnNAYT6yjci"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "# try adam and sgd \n",
    "# do a list for learning rate as well - hyperparamter tuning\n",
    "model_naive.compile(loss='categorical_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=1e-2),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRvM_pEZy7SX"
   },
   "source": [
    "[6.3] **TODO** Fit your model using the number of epochs defined. SAve the ouput to a variable called `history`. You can set up some callbacks if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMzFo2r5JKn6"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "\n",
    "# adding callbacks\n",
    "\n",
    "# reduce learning rate if val_loss reduction hits a limit\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.00001)\n",
    "\n",
    "# implement early stopping to limit time spent training and over fitting\n",
    "earlystoppings = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10) # patience larger than reduceLR\n",
    "\n",
    "# save he best model loaded in later steps\n",
    "modelcheckpoint = ModelCheckpoint('best_model_naive.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# train #\n",
    "model_naive.fit(x_train, y_train,\n",
    "          batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(x_test, y_test),callbacks=[reduce_lr,earlystoppings,modelcheckpoint])\n",
    "\n",
    "# save history\n",
    "history_naive = model_naive.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNvB5AaC6dTL"
   },
   "source": [
    "[6.4] Save the weights of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rmtkn07hk0hU"
   },
   "outputs": [],
   "source": [
    "model_naive.save_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "# load the saved model\n",
    "model_naive = load_model('best_model_naive.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vz9uFy_X6oeA"
   },
   "source": [
    "# 7. Analyse Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndbhokSn4_vU"
   },
   "source": [
    "Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoiH1Z0z7foN"
   },
   "outputs": [],
   "source": [
    "# functions to display results\n",
    "\n",
    "def print_results(model,x_train, y_train,x_test, y_test):\n",
    "\n",
    "  # evaluate model on both train and test sets\n",
    "  # ouputs are score and accuracy\n",
    "  # get for both train and test set\n",
    "  score_train = model.evaluate(x_train, y_train)\n",
    "  print('train score:', score_train[0])\n",
    "  print('train accuracy:', score_train[1]) \n",
    "\n",
    "  score_test = model.evaluate(x_test, y_test)\n",
    "  print('test score:', score_test[0])\n",
    "  print('test accuracy:', score_test[1]) \n",
    "  \n",
    "\n",
    "\n",
    "def plot_results(model):\n",
    "    # use input to get model data containing metrics\n",
    "    # plot metrics for both train and test on accuracy and loss\n",
    "    history = model\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def create_confusion_matrix(model,x_test,y_test):\n",
    "  \n",
    "    # confusion matrix\n",
    "    # run prediction on x_test and compare to y_test (true labels)\n",
    "    predictions = model.predict(x_test)\n",
    "    y_pred_argmax = np.argmax(predictions, axis=1)\n",
    "    y_test_argmax = np.argmax(y_test, axis=1)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test_argmax, y_pred_argmax)\n",
    "\n",
    "    # Plot Confusion matrix use sns to add colour\n",
    "    sns.heatmap(conf_matrix.T, square=True, annot=True, cbar=False, cmap=plt.cm.Blues)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('True Values');\n",
    "    plt.show();   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddugPZhZ68Wb"
   },
   "source": [
    "[7.1] **TODO** Display the performance of your model on the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAr6yvokDfMp"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "print_results(model_naive,x_train, y_train,x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBTo_xEI7K_z"
   },
   "source": [
    "[7.2] **TODO** Plot the learning curve of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRt_4W2F7RVV"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "plot_results(history_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKPu98GR7a17"
   },
   "source": [
    "[7.3] **TODO** Display the confusion matrix on the testing set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkrP9JCgMzpT"
   },
   "outputs": [],
   "source": [
    "# TODO (Students need to fill this section)\n",
    "create_confusion_matrix(model_naive,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FEjinuu3g1U"
   },
   "source": [
    "# Experiment 2: Adding layers from 2 up to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXIACfPh5MhO"
   },
   "source": [
    "4 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f43AcJ8k3n_J"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping=1\n",
    "# Same model as naive but wit 4 layers\n",
    "model_naive_4_layers = Sequential() # seems to be widely used\n",
    "\n",
    "# Input layer  \n",
    "model_naive_4_layers.add(keras.Input(shape=(28, 28, 1)))\n",
    "model_naive_4_layers.add(Flatten())\n",
    "\n",
    "# 1st hidden layer\n",
    "model_naive_4_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  2nd hidden layer\n",
    "model_naive_4_layers.add(Dense(512, activation=\"relu\"))\n",
    "# 3rd hidden layer\n",
    "model_naive_4_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  4th hidden layer\n",
    "model_naive_4_layers.add(Dense(512, activation=\"relu\"))\n",
    "\n",
    "# Final  layer\n",
    "model_naive_4_layers.add(Dense(num_classes,activation=\"softmax\"))\n",
    "\n",
    "# compile model\n",
    "model_naive_4_layers.compile(loss='categorical_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=1e-2),metrics=['accuracy'])\n",
    "\n",
    "# save he best model loaded in later steps\n",
    "modelcheckpoint = ModelCheckpoint('best_model_naive_4_layers.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# train #\n",
    "model_naive_4_layers.fit(x_train, y_train,\n",
    "          batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(x_test, y_test),callbacks=[reduce_lr,earlystoppings,modelcheckpoint])\n",
    "\n",
    "# save history\n",
    "history_naive_4_layers = model_naive_4_layers.history\n",
    "\n",
    "# print results\n",
    "print_results(model_naive_4_layers,x_train, y_train,x_test, y_test)\n",
    "\n",
    "plot_results(history_naive_4_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHD11O-p2vpO"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping=1\n",
    "# show model\n",
    "model_naive_4_layers.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wukSxChW5Su1"
   },
   "source": [
    "6 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgWJdw2A47p3"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping=1\n",
    "# Same model as naive but with 6 layers\n",
    "model_naive_6_layers = Sequential() # seems to be widely used\n",
    "\n",
    "# Input layer  \n",
    "model_naive_6_layers.add(keras.Input(shape=(28, 28, 1)))\n",
    "model_naive_6_layers.add(Flatten())\n",
    "\n",
    "# 1st hidden layer\n",
    "model_naive_6_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  2nd hidden layer\n",
    "model_naive_6_layers.add(Dense(512, activation=\"relu\"))\n",
    "# 3rd hidden layer\n",
    "model_naive_6_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  4th hidden layer\n",
    "model_naive_6_layers.add(Dense(512, activation=\"relu\"))\n",
    "# 5th hidden layer\n",
    "model_naive_6_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  6th hidden layer\n",
    "model_naive_6_layers.add(Dense(512, activation=\"relu\"))\n",
    "# Final  layer\n",
    "model_naive_6_layers.add(Dense(num_classes,activation=\"softmax\"))\n",
    "\n",
    "# compile model\n",
    "model_naive_6_layers.compile(loss='categorical_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=1e-2),metrics=['accuracy'])\n",
    "\n",
    "# save he best model loaded in later steps\n",
    "modelcheckpoint = ModelCheckpoint('best_model_naive_6_layers.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# train #\n",
    "model_naive_6_layers.fit(x_train, y_train,\n",
    "          batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(x_test, y_test),callbacks=[reduce_lr,earlystoppings,modelcheckpoint])\n",
    "\n",
    "# save history\n",
    "history_naive_6_layers = model_naive_6_layers.history\n",
    "\n",
    "# print results\n",
    "print_results(model_naive_6_layers,x_train, y_train,x_test, y_test)\n",
    "\n",
    "plot_results(history_naive_6_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x47nctcO3CQ_"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping=1\n",
    "# show model\n",
    "model_naive_6_layers.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3GTnBiu5kOl"
   },
   "source": [
    "8 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Zq4Z5_J5hPo"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping=1\n",
    "# Same model as naive but with 8 layers\n",
    "model_naive_8_layers = Sequential() # seems to be widely used\n",
    "\n",
    "# Input layer  \n",
    "model_naive_8_layers.add(keras.Input(shape=(28, 28, 1)))\n",
    "model_naive_8_layers.add(Flatten())\n",
    "\n",
    "# 1st hidden layer\n",
    "model_naive_8_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  2nd hidden layer\n",
    "model_naive_8_layers.add(Dense(512, activation=\"relu\"))\n",
    "# 3rd hidden layer\n",
    "model_naive_8_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  4th hidden layer\n",
    "model_naive_8_layers.add(Dense(512, activation=\"relu\"))\n",
    "# 5th hidden layer\n",
    "model_naive_8_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  6th hidden layer\n",
    "model_naive_8_layers.add(Dense(512, activation=\"relu\"))\n",
    "# 7th hidden layer\n",
    "model_naive_8_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  8th hidden layer\n",
    "model_naive_8_layers.add(Dense(512, activation=\"relu\"))\n",
    "# Final  layer\n",
    "model_naive_8_layers.add(Dense(num_classes,activation=\"softmax\"))\n",
    "\n",
    "# compile model\n",
    "model_naive_8_layers.compile(loss='categorical_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=1e-2),metrics=['accuracy'])\n",
    "\n",
    "# save he best model loaded in later steps\n",
    "modelcheckpoint = ModelCheckpoint('best_model_naive_8_layers.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# train #\n",
    "model_naive_8_layers.fit(x_train, y_train,\n",
    "          batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(x_test, y_test),callbacks=[reduce_lr,earlystoppings,modelcheckpoint])\n",
    "\n",
    "# save history\n",
    "history_naive_8_layers = model_naive_8_layers.history\n",
    "\n",
    "# print results\n",
    "print_results(model_naive_8_layers,x_train, y_train,x_test, y_test)\n",
    "\n",
    "plot_results(history_naive_8_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pV9dVpKg3MrA"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping=1\n",
    "# show model\n",
    "model_naive_8_layers.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnDnPM315qQN"
   },
   "source": [
    "10 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9De5QuEQ5wqL"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping=1\n",
    "# Same model as naive but with 8 layers\n",
    "model_naive_10_layers = Sequential() # seems to be widely used\n",
    "\n",
    "# Input layer  \n",
    "model_naive_10_layers.add(keras.Input(shape=(28, 28, 1)))\n",
    "model_naive_10_layers.add(Flatten())\n",
    "\n",
    "# 1st hidden layer\n",
    "model_naive_10_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  2nd hidden layer\n",
    "model_naive_10_layers.add(Dense(512, activation=\"relu\"))\n",
    "# 3rd hidden layer\n",
    "model_naive_10_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  4th hidden layer\n",
    "model_naive_10_layers.add(Dense(512, activation=\"relu\"))\n",
    "# 5th hidden layer\n",
    "model_naive_10_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  6th hidden layer\n",
    "model_naive_10_layers.add(Dense(512, activation=\"relu\"))\n",
    "# 7th hidden layer\n",
    "model_naive_10_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  8th hidden layer\n",
    "model_naive_10_layers.add(Dense(512, activation=\"relu\"))\n",
    "# 9th hidden layer\n",
    "model_naive_10_layers.add(Dense(512, activation=\"relu\"))\n",
    "#  10th hidden layer\n",
    "model_naive_10_layers.add(Dense(512, activation=\"relu\"))\n",
    "# Final  layer\n",
    "model_naive_10_layers.add(Dense(num_classes,activation=\"softmax\"))\n",
    "\n",
    "# compile model\n",
    "model_naive_10_layers.compile(loss='categorical_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=1e-2),metrics=['accuracy'])\n",
    "\n",
    "# save he best model loaded in later steps\n",
    "modelcheckpoint = ModelCheckpoint('best_model_naive_10_layers.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# train #\n",
    "model_naive_10_layers.fit(x_train, y_train,\n",
    "          batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(x_test, y_test),callbacks=[reduce_lr,earlystoppings,modelcheckpoint])\n",
    "\n",
    "# save history\n",
    "history_naive_10_layers = model_naive_10_layers.history\n",
    "\n",
    "# print results\n",
    "print_results(model_naive_10_layers,x_train, y_train,x_test, y_test)\n",
    "\n",
    "plot_results(history_naive_10_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFvjVXed3Z3z"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping=1\n",
    "# show model\n",
    "model_naive_10_layers.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBJZjNcmh3EW"
   },
   "source": [
    "# Experiment 3: Adding dropout to naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBqzHOeZc1nV"
   },
   "outputs": [],
   "source": [
    "# adding drop out of 20% to naive model\n",
    "model_naive_dropout = Sequential() # seems to be widely used\n",
    "\n",
    "# Input layer  \n",
    "model_naive_dropout.add(keras.Input(shape=(28, 28, 1)))\n",
    "model_naive_dropout.add(Flatten())\n",
    "\n",
    "# 1st hidden layer\n",
    "model_naive_dropout.add(Dense(512, activation=\"relu\"))\n",
    "model_naive_dropout.add(Dropout(0.2)) # dropout 20%\n",
    "\n",
    "#  2nd hidden layer\n",
    "model_naive_dropout.add(Dense(512, activation=\"relu\"))\n",
    "model_naive_dropout.add(Dropout(0.2)) # dropout 20%\n",
    "\n",
    "# Final  layer\n",
    "model_naive_dropout.add(Dense(num_classes,activation=\"softmax\"))\n",
    "\n",
    "# Compile\n",
    "model_naive_dropout.compile(loss='categorical_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=1e-2),metrics=['accuracy'])\n",
    "\n",
    "# rename saved file for best model\n",
    "modelcheckpoint = ModelCheckpoint('best_model_naive_dropout.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "# Fit model\n",
    "model_naive_dropout.fit(x_train, y_train,\n",
    "          batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(x_test, y_test),callbacks=[reduce_lr,earlystoppings,modelcheckpoint])\n",
    "\n",
    "# save history\n",
    "history_naive_dropout = model_naive_dropout.history\n",
    "\n",
    "# save weights\n",
    "model_naive_dropout.save_weights('./checkpoints/my_checkpoint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Sq_UYPpf8Qc"
   },
   "outputs": [],
   "source": [
    "# get summary\n",
    "model_naive_dropout.summary()\n",
    "\n",
    "# plot model_naive \n",
    "plot_model(model_naive_dropout, to_file='model_dropout_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tga811rdfoQp"
   },
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "model_naive_dropout = load_model('best_model_naive_dropout.h5')\n",
    "\n",
    "#print results\n",
    "print_results(model_naive_dropout,x_train, y_train,x_test, y_test)\n",
    "\n",
    "#plot results\n",
    "plot_results(history_naive_dropout)\n",
    "\n",
    "# confusion matrix\n",
    "create_confusion_matrix(model_naive_dropout,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCZvER7Uhsbc"
   },
   "source": [
    "# Experiment 3 Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtf7AkMUTlPU"
   },
   "outputs": [],
   "source": [
    "# since working with images, l'm trying try a Convolutional Neural Network\n",
    "# kernel is 3x3 as its cheaper to implements\n",
    "# Also adds more non linearity which may help\n",
    "# activation function is relu except for final layer\n",
    "# based off mnist model that achived 97% acccracy\n",
    "# batch normalisation for regularisation and limit overfitting - need to justify usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsFxgNKU9Cum"
   },
   "outputs": [],
   "source": [
    "# model architecture\n",
    "# no initial flattening as we need to perform convolutions on each image befoire flattening it\n",
    "model_cnn_1=Sequential()\n",
    "  \n",
    "model_cnn_1.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(28,28,1))) # input as images + first kernel\n",
    "model_cnn_1.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\")) # second 3x3 kernel \n",
    "\n",
    "model_cnn_1.add(MaxPooling2D(pool_size=(2,2))) # max pool to reduce paramaters\n",
    "model_cnn_1.add(BatchNormalization()) # add batch normalisation to hopefully improve training speed and reduce overfitting\n",
    "model_cnn_1.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))  # first of two kernels at 128x128\n",
    "model_cnn_1.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
    "\n",
    "model_cnn_1.add(MaxPooling2D(pool_size=(2,2))) # max pool again\n",
    "model_cnn_1.add(BatchNormalization())    \n",
    "model_cnn_1.add(Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\")) # last kernel\n",
    "\n",
    "\n",
    "model_cnn_1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# flatten for dense layer     \n",
    "model_cnn_1.add(Flatten())\n",
    "model_cnn_1.add(BatchNormalization())\n",
    "model_cnn_1.add(Dense(512,activation=\"relu\"))\n",
    "# final layer for prediction using softmax  \n",
    "model_cnn_1.add(Dense(10,activation=\"softmax\"))\n",
    "\n",
    "# compile model\n",
    "model_cnn_1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# train model\n",
    "modelcheckpoint = ModelCheckpoint('best_model_cnn_1.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model_cnn_1.fit(x_train, y_train, batch_size=batch_size, validation_data=(x_test, y_test), epochs=epochs,verbose=1, callbacks=[reduce_lr,earlystoppings,modelcheckpoint])\n",
    "\n",
    "#save hisotry\n",
    "history_cnn_1 = model_cnn_1.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUsQW9awEPKb"
   },
   "outputs": [],
   "source": [
    "# plot cnn model architecture\n",
    "\n",
    "# visualise model\n",
    "!pip install visualkeras\n",
    "from PIL import ImageFont\n",
    "import visualkeras\n",
    "\n",
    "model_cnn_1.summary()\n",
    "\n",
    "visualkeras.layered_view(model_cnn_1, legend=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp-oChCSzIqv"
   },
   "outputs": [],
   "source": [
    "#keras plot model\n",
    "plot_model(model_cnn_1, to_file='model_cnn_1.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQSOpRWqhbr3"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model_cnn_1 = load_model('best_model_cnn_1.h5')\n",
    "\n",
    "print_results(model_cnn_1,x_train, y_train,x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LFX2KpYhVJS"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model_cnn_1 = load_model('best_model_cnn_1.h5')\n",
    "\n",
    "print_results(model_cnn_1,x_train, y_train,x_test, y_test)\n",
    "\n",
    "# cnn plot results\n",
    "plot_results(history_cnn_1)\n",
    "\n",
    "# cnn confusion matrix\n",
    "create_confusion_matrix(model_cnn_1,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKE3odNUhj-x"
   },
   "source": [
    "# Synthetic data genenerator for CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIawDkfJuC1T"
   },
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "\n",
    "# With data augmentation to prevent overfitting\n",
    "# try rotating image as some people may write slanted\n",
    "# trying zoom  as well as thy may write larger/smaller than average\n",
    "# also adding shifting incase image isn't centered\n",
    "# no flipping is necessary as its poitnless in this case due to edpected nature of written letters having defined directionality\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "\n",
    "        rotation_range=15,  # randomly rotate images in the range \n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        shear_range=0.3  # adds distortion to image\n",
    " \n",
    "        )\n",
    "\n",
    "#datagen.fit(x_train)\n",
    "train_data_aug = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "#test_data_aug = datagen.flow(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ktgoeaZubsU"
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "model_cnn_data_aug=model_cnn_1 # use same architecture\n",
    "\n",
    "modelcheckpoint = ModelCheckpoint('best_model_cnn_data_aug.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model_cnn_data_aug.fit(train_data_aug, batch_size=batch_size, validation_data=(x_test, y_test), epochs=100,verbose=1,callbacks=[reduce_lr,earlystoppings,modelcheckpoint] )\n",
    "#validation_data=(test_data_aug)\n",
    "history_cnn_data_aug = model_cnn_data_aug.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gA0D1mMrubyh"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model_cnn_data_aug = load_model('best_model_cnn_data_aug.h5')\n",
    "\n",
    "# print score \n",
    "score_train_cnn_data_aug = model_cnn_data_aug.evaluate(train_data_aug)\n",
    "print('train score:', score_train_cnn_data_aug[0])\n",
    "print('train accuracy:', score_train_cnn_data_aug[1]) \n",
    "\n",
    "score_test_cnn_data_aug = model_cnn_data_aug.evaluate(x_test, y_test)\n",
    "print('test score:', score_test_cnn_data_aug[0])\n",
    "print('test accuracy:', score_test_cnn_data_aug[1]) # 93% accurate accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Enp50fKrub2a"
   },
   "outputs": [],
   "source": [
    "# plot results for data augmentation\n",
    "plot_results(history_cnn_data_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2reDtIp9uqXF"
   },
   "outputs": [],
   "source": [
    "# data augmentation confusion matrix\n",
    "create_confusion_matrix(model_cnn_data_aug,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0D6F6f_jKnGM"
   },
   "outputs": [],
   "source": [
    "def predict_show_classes(model, x_val, y_val):\n",
    "\n",
    "    #get the predictions for the test data using model input\n",
    "    y_predict = np.argmax(model.predict(x_val), axis=-1)\n",
    "\n",
    "    #get the values to be plotted using argmax\n",
    "\n",
    "    y_true = np.argmax(y_val,axis=1)\n",
    "    correct = np.nonzero(y_predict==y_true)[0]\n",
    "    incorrect = np.nonzero(y_predict!=y_true)[0]\n",
    "\n",
    "    # print out results\n",
    "    print(\"Correct predicted classes:\",correct.shape[0])\n",
    "    print(\"Incorrect predicted classes:\",incorrect.shape[0])\n",
    "\n",
    "    # show metrics for classes\n",
    "    target_names = [\"Class {}:\".format(i) for i in range(10)]\n",
    "    print(classification_report(y_true, y_predict, target_names=target_names))\n",
    "    return correct, incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Zn-fUtnohIj"
   },
   "outputs": [],
   "source": [
    "# getting metrics model from sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "correct, incorrect = predict_show_classes(model_naive, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuQa7HM4iEPc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy_of_Assignment1_PartB_Instruction.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
